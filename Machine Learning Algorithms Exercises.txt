Part I.
a) Use three features:
	1) Commands
		> data<-gold_target1
		> data
		> data[,4]<-factor(data[,4],labels=c("NO","Yes"))
		> data[,4]
		> m<-naiveBayes(data[,1:3],data[,4])
		> predict(m,data[,1:3])
		[1] NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  Yes NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO 
[31] NO  NO  Yes NO  NO  Yes NO  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes
[61] Yes Yes Yes Yes
Levels: NO Yes
		> table(predict(m,data[,1:3]),data[,4])
		
	2) The table resulting from the evaluation of the model
			NO Yes
		NO  33   1
		Yes  3  27
		
	3) Evaluation:
		
b)	Use two features:
	1) Commands
		> m<-naiveBayes(data[,1:2],data[,4]) 
		> predict(m,data[,1:2])
		[1] NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO  NO 
[31] NO  NO  NO  NO  NO  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes NO  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes
[61] Yes Yes Yes Yes
Levels: NO Yes
		> table(predict(m,data[,1:2]),data[,4])
		
	2) The table resulting from the evaluation of the model	
	    NO Yes
	NO  34   2
	Yes  2  26
	
	3) Evaluation:
		
c) Use one feature:
	1) Commands
		> m<-naiveBayes(data[,1],data[,4])
		> predict(m,data[,1])
		[1] NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO
[41] NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO NO
Levels: NO Yes
		> table(predict(m,data[,1]),data[,4])
		
    2) The table resulting from the evaluation of the model
		NO Yes
	NO  36  28
	Yes  0   0
	
	3) Evaluation:
	

Part II.
a) minsplit = 2:
	1) Commands
	> library(rpart)
	> fit = rpart(V4 ~ V1 + V2 + V3, method = "class", data=data, control =rpart.control(minsplit=2, cp=0.001))
	> dt1 = rpart(V4 ~ V1 + V2 + V3, method = "class", data=data, control =rpart.control(minsplit=2, cp=0.001))
	> print(dt1)	
	> plot(dt1, uniform=TRUE, main="Classification Tree for Gold Dataset")
	> text(dt1, use.n=TRUE, all=TRUE, cex=.7)
	> table(predict(dt1, data, type="class"), data$V4)
    
	2) The table resulting from the evaluation of the model
		NO Yes
	NO  36   0
	Yes  0  28

	3) Evaluation
	
b) minsplit = 3:
	1) Commands
	> dt2 = rpart(V4 ~ V1 + V2 + V3, method = "class", data=data, control =rpart.control(minsplit=3, cp=0.001))
	> print(dt2)
	> plot(dt2, uniform=TRUE, main="Classification Tree for Gold Dataset")
	> text(dt2, use.n=TRUE, all=TRUE, cex=.7)
	> table(predict(dt2, data, type="class"), data$V4)

    2) The table resulting from the evaluation of the model 
		NO Yes
	NO  35   1
	Yes  1  27
	
	3) Evaluation
	
b) minsplit = 4:
	1) Commands
	> dt3 = rpart(V4 ~ V1 + V2 + V3, method = "class", data=data, control =rpart.control(minsplit=4, cp=0.001))
	> print(dt3)
	> plot(dt3, uniform=TRUE, main="Classification Tree for Gold Dataset")
	> text(dt3, use.n=TRUE, all=TRUE, cex=.7)
	> table(predict(dt3, data, type="class"), data$V4)
	
	2) The table resulting from the evaluation of the model
	    NO Yes
	NO  35   2
	Yes  1  26
	
	3) Evaluation